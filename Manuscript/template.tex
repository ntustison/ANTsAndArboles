%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}

\graphicspath{
             {./Figures/}
             }


%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}

\usepackage{color,bm,natbib}
\usepackage[colorlinks=true,breaklinks]{hyperref}
\usepackage{multirow,booktabs,ctable,array}
\usepackage{amsmath}
\usepackage{amssymb}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Optimal Symmetric Multimodal Templates and Concatenated Random Forests for Supervised Brain Tumor Segmentation (Simplified) with \textit{ANTsR}%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Nicholas J.~Tustison \and 
        K.~L.~Shrinidhi \and
        Max Wintermark \and
        Christopher R.~Durst \and
        Benjamin M.~Kandel \and
        James C.~Gee \and
        Murray C.~Grossman \and
        Brian B.~Avants
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{N. Tustison \at
              Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA \\
%              Tel.: +123-45-678910\\
%              Fax: +123-45-678910\\
              \email{ntustison@virginia.edu}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
              \and
              K. Shrinidhi, B. Kandel, J. Gee, and M. Grossman \at
              Penn Image Computing and Science Laboratory, Department of Radiology, University of Pennsylvania, Philadelphia, PA
              \and
              M. Wintermark, C. Durst \at
              Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA \\
              \and
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Segmenting and quantifying gliomas from MRI is an important task for diagnosis, planning intervention, 
and for tracking tumor changes over time.
However, this task is complicated by the lack
of prior knowledge concerning tumor location, spatial extent, shape,
possible displacement of normal tissue, and intensity signature.
%These factors limit the applicability of techniques with established
%performance in related tasks such as normal brain tissue
%segmentation.  
To accommodate such complications, we introduce a framework for supervised
segmentation based on multiple modality intensity, geometry, and asymmetry feature sets.  These features drive a supervised whole-brain and tumor segmentation
approach based on random forest-derived probabilities.  The
asymmetry-related features (based on optimal symmetric multimodal 
templates) demonstrate excellent discriminative 
properties within this framework. We also gain performance by  
generating probability maps from random forest models and using these maps 
for a refining Markov random field regularized probabilistic segmentation.  
This strategy allows us to interface the supervised learning capabilities 
of the random forest model with regularized probabilistic segmentation
using the recently developed \textit{ANTsR} 
package---a comprehensive statistical and visualization interface between the
popular Advanced Normalization Tools (ANTs) and the \textit{R}
statistical project. 
%\textit{ANTsR} integrates proven image analysis methods
%for image registration, segmentation, bias correction, and
%template building with \textit{R}'s statistical and machine
%learning techniques and state-of-the-art visualization possibilities.  
The reported algorithmic framework was the
top-performing entry in the MICCAI 2013 Multimodal Brain Tumor
Segmentation challenge.  The challenge data were widely varying 
consisting of both high-grade and low-grade glioma tumor 
four-modality MRI from five different institutions.
Average Dice overlap measures for the final algorithmic assessment
were 0.87, 0.78, and 0.74 for ``complete'', ``core'', and ``enhanced'' 
tumor components, respectively.
%and, to our knowledge, is the only entry for
%both challenge years (2012 \& 2013) which has been made publicly
%available.
\keywords{advanced normalization tools \and BRATS \and glioma \and \textit{R} project
}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}

Given the complexity of tumor growth and appearance and the need
for precise volumetric measurements for tumor characterization, 
much research has been invested in computational methods for 
automated segmentation of tumor regions in MR images.  Several 
approaches have been previously proposed in the literature as
detailed in recent reviews \citep{angelini2007,bauer2013}.  
Commensurable algorithmic evaluation, however, is
extremely problematic due to widely varying performance assessments
described in the corresponding publications, lack of publicly available 
evaluation data, and private algorithmic instantiations.  
In response to these issues, the Multimodal Brain Tumor Segmentation 
(BRATS) challenge was initiated in 2012 (and continued in 2013) under 
the auspices of the Medical Image Computing and Computer Assisted 
Intervention Society in association with their annual international 
conference \citep{menze2014}.

After observing the 2012 challenge to gain insight for our own research 
needs \citep{durst2014}, we found few implementation resources available.
  This prompted our participation in the challenge which took place
the following year in Nagoya, Japan concerning brain tumor segmentation
of a varied data set from five different institutions involving FLAIR, 
T1-weighted, T1-weighted contrast (T1C), and T2-weighted MRI modalities.  Heavily influenced by the success 
of participants in the 2012 challenge (specifically, the work of 
\cite{bauer2012,geremia2012,zikic2012}), we adopted the increasingly popular 
random forest (RF) machine learning framework \citep{breiman2001} which 
permits the inclusion of many potentially discriminative image features.  
Supervised segmentation techniques generally consist of a training phase
for model construction using image-based feature data followed by prediction using the 
generated model.  For supervised brain tumor segmentation, 
a set of training data consisting of labeled brain images 
is used to construct a predictive model.  Although other 
classification techniques have been used to segment
brain tumors (e.g., support vector machines \citep{bauer2011}),
RF models have proven particularly successful.

Several machine learning concepts were integrated to create 
the RF framework first articulated in its entirety by Breiman
et al. \citep{breiman2001} for performing classification/regression.  
Although decision trees had been previously explored in the literature, 
it was the success of ``boosting''-style machine learning 
techniques, such as AdaBoost \citep{schapire1990,freund1997}, which influenced 
the aggregation of such decision trees into ``forests'' 
with randomized node optimization for improved
classification/regression performance \citep{ho1995,amit1997}.
The final element of bootstrap aggregating or ``bagging'' (i.e.
random sampling of the training data) was
introduced by Breiman \citep{breiman1996} to achieve improved
accuracy.%
%\footnote{
%One of the principal advantages of \textit{R} is the extensive community of
%developers  who have contributed on the order of thousands of packages 
%extending \textit{R} 's capabilities beyond its core functionality.
%Most relevant here
%is the {\tt randomForest} package developed from Breiman's original
%Fortran code by Liaw and Wiener \citep{liaw2002}.
%}

Early adoption \citep{viola2005} and success in the
computer vision community
has led to a recent surge within the medical image analysis
community of using RFs for handling complex 
classification/regression tasks including
normal brain segmentation \citep{yi2009},
MS lesion segmentation \cite{geremia2011}, 
multimodal brain tumor segmentation
\citep{bauer2012,zikic2012}, brain extraction \citep{iglesias2010}, 
classification of Alzheimer's disease \citep{gray2013},
anatomy detection in computed tomography \citep{criminisi2013}, and
segmentation of echocardiographic images \citep{verhoek2011}. 
A thorough introduction for those interested in delving deeper 
into the more theoretical aspects of RFs can be found
in \cite{criminisi2011}.

%One of the innovations that we provide in this work is
%the use of \textit{concatenated RFs}  for improved probabilistic estimation
%of the brain segmentation labels.  Briefly, Gaussian mixture
%modeling (GMM) is used to create a subset of feature images which are
%used to train and predict during the first classification stage.  
%The RF voting output probabilistic maps are then used as 
%spatial probability priors for performing a MAP-MRF segmentation
%and subsequent feature image creation
%thereby replacing the GMM-derived feature images for a second
%classification stage.  This dual RF model generation
%and application improves results and is detailed in later sections.


Instrumental to our success in the 2013 challenge 
was the use of image features based on multimodal shape and appearance 
asymmetries which were generated via the construction of symmetric
multivariate brain templates.  Additional improvements in performance
were due to the use of a two-stage RF model approach
whereby the output probability images from the application of the first
RF model seeds the construction of a subset of new feature images
for input into the second stage.  
%The results of the second RF model application
%are then refined using a series of heuristically-derived binary morphological
%operations.  
The two-stage approach is motivated by
the fact that the RF is a powerful tool for ranking the features
in a high-dimensional model i.e., it can be used for model selection
in voxel-wise classification.  However, the RF outputs are not geometrically 
constrained, which is the motivation for our second refinement step.

In line with our philosophy regarding open science \citep{tustison2013,ince2012}, 
we have made the code available as open source
%\footnote{
%https://github.com/ntustison/BRATS2013
%}
and, to our knowledge, we are the only competitors out of the approximately 20 teams 
that participated in either years' challenges to do this.  All code is based
on the well-vetted and open source Insight Toolkit (ITK)
%\footnote{
%http://www.itk.org/
%} 
of the National Institutes of Health and is available through our \textit{ANTsR} 
package.
In subsequent sections we describe in greater detail the automated 
supervised brain segmentation pipeline including the generation of asymmetry
feature images and the concatenated RF construction.  We also
describe the code and data used for the BRATS challenge so that 
the interested reader can reproduce our results.  Finally, we 
illustrate performance with results from the BRATS 2013 challenge.
 
%\footnote{
%Although all data was provided having been skull-stripped, 
%we note to the reader that an
%available ANTs-based solution to the brain extraction problem
%is found in the script {\tt antsBrainExtraction.sh} which uses
%a hybrid approach based on image registration and binary morphology
%to isolate the brain \citep{avants2010a}.  An evaluation with other
%methodologies demonstrated comparable to superior performance 
%\citep{avants2010a}.
%} 


\section{Materials and Methods}

Feature generation for the proposed framework is critical.  The performance
of the selected machine learning strategy is limited by the input used for 
training. The image features described below were selected based on previous
work in the literature and our own observations regarding tumor characteristics
and what could possibly be encoded algorithmically.  Observations 
regarding tumor intensity differences from normal tissue has been encoded
in previously proposed research by calculating normalized neighborhood 
intensity statistics and intensity modeling \citep[e.g.,][]{bauer2011,bauer2012,geremia2012,zikic2012}.
In these approaches both the features and classification algorithm are limited to 
voxelwise considerations.  

Our success in the MICCAI 2013 challenge was largely due to the inclusion of
an extremely important set of features based on the common observation of
bilateral asymmetrization caused by tumor presence.  Additionally, several
features were designed to encode spatial coherence even though the RF classification
is performed on a voxelwise basis. For example, connected component geometry of the
intensity modeling labels were used to discriminate observations of compact,
isotropic objects (vs. highly anisotropic) typical of tumor presence and growth 
\citep{greenspan1972}.  Table \ref{table:features} provides a listing of image
features used and the motivation for their inclusion.  

Certain key elements characterize the proposed supervised brain tumor segmentation
protocol including:
\begin{itemize}
  \item construction of symmetric multivariate templates,
  \item generation of image features including those based on:
    \begin{itemize}
      \item asymmetry and template normalization, 
      \item intensity modeling and connected component geometry,      
      \item neighborhood first-order statistics, and
      \item brain mask coordinate systems.
    \end{itemize}
  \item training and geometric refinement of RF models, and
  \item prediction using the proposed framework.
\end{itemize}
In subsequent sections, we describe these items and detail the
\textit{ANTsR} framework which coordinates all steps.

\begin{table}
\caption{Image feature list for the proposed supervised brain tumor segmentation.}
\label{table:features}
\centering
\begin{tabular*}{0.99\textwidth}{@{\extracolsep{\fill}} c c l}
\toprule
\multicolumn{3}{c}{\bf Asymmetry and template normalization} \\
\midrule
\multicolumn{1}{c}{Feature} & \multicolumn{1}{c}{Number} & \multicolumn{1}{c}{Motivation} \\
\midrule
contralateral difference & 1 per modality & contralateral comparison \\
template difference  & 1 per modality & average normal comprison\\
log Jacobian &  1 & tumor distortion \\
warped template distance & 1 & tumor distortion \\
\midrule
\multicolumn{3}{c}{\bf Intensity modeling and connected component geometry} \\
\midrule
\multicolumn{1}{c}{Feature} & \multicolumn{1}{c}{Number} & \multicolumn{1}{c}{Motivation} \\
\midrule
Pr(CSF)  & 1 per modality & normal tissue intensity\\
Pr(gray matter) & 1 per modality & normal tissue intensity\\
Pr(white matter) & 1 per modality & normal tissue intensity\\
Pr(edema) & 1 per modality & normal tissue intensity\\
Pr(non-enhancing) & 1 per modality & tumor tissue intensity\\
Pr(enhancing) & 1 per modality & tumor tissue intensity\\
Pr(necrosis) & 1 per modality & tumor tissue intensity\\
elongation & 1 per modality & anisotropic components \\
eccentricity & 1 per modality & anisotropic components \\
volume & 1 per modality & small, isolated components \\
distance to tumor core & 1 per modality & proximity to tumor core \\
$\frac{\mathrm{volume}}{\mathrm{surface\,\,area}}$ & 1 per modality & anisotropic components \\
\midrule
\multicolumn{3}{c}{\bf Neighborhood first-order statistics} \\
\midrule
\multicolumn{1}{c}{Feature} & \multicolumn{1}{c}{Number} & \multicolumn{1}{c}{Motivation} \\
\midrule
mean (radius = 1)  & 1 per modality &  tissue \& tumor intensity \\
std. dev. (radius = 1)  & 1 per modality & tissue \& tumor  intensity \\
skewness (radius = 1)  & 1 per modality & tissue \& tumor  intensity \\
mean (radius = 3)  & 1 per modality & tissue \& tumor  intensity \\
std. dev. (radius = 3)  & 1 per modality & tissue \& tumor  intensity \\
skewness (radius = 3)  & 1 per modality & tissue \& tumor  intensity \\
T1, T1C difference & 1 & isolate tumor tissue \\
\midrule
\multicolumn{3}{c}{\bf Brain mask coordinate system} \\
\midrule
\multicolumn{1}{c}{Feature} & \multicolumn{1}{c}{Number} & \multicolumn{1}{c}{Motivation} \\
\midrule
subject distance & 1 & peripheral vs. subcortical \\
\bottomrule
\end{tabular*}
\end{table}






%FeatureSet ,  Number of Features ,   Motivation  ,  Modalities involved
%
%Asymmetry ,       2                        ,    ....            ,






\subsection{Optimal Symmetric Multivariate Templates}

\setlength{\tabcolsep}{2pt}



In order to better characterize deviations from normal brain shape 
and appearance, several image features were derived using symmetric 
population-specific multivariate templates.  
For normal neuroanatomy, the use of spatial prior information 
coupled with image normalization capabilities has proven useful 
in producing improved segmentation results of ``expected'' brain tissue
such as cerebrospinal fluid, gray matter, and white matter 
\citep[e.g.,][]{ashburner1997}.  In contrast, accommodating spatial 
priors to model the presence of a possible tumor and its constituent tissue 
components is difficult. However, since the normal brain 
exhibits a bilaterally symmetric organization, we can 
use the presence of asymmetries to potentially differentiate abnormal 
brain tissue.  A similar motivation prompted
the identification of the mid-sagittal plane of symmetry \citep{prima2002}
for feature generation in multiple sclerosis lesion \citep{geremia2011} and 
tumor \citep{geremia2012} identification.  However, this earlier
approach does not take into account the displacement of 
normal tissue due to tumor growth causing the mid-sagittal plane
to deform from its planar structure (cf Figure \ref{fig:asymmetry}).

\begin{figure}
  \centering
  \begin{tabular}{cccc}
  \includegraphics[width=28mm]{Fig1a.eps} &
  \includegraphics[width=28mm]{Fig1b.eps} &
  \includegraphics[width=28mm]{Fig1c.eps} &
  \includegraphics[width=28mm]{Fig1d.eps} \\
  (a) & (b) &
  (c) & (d) \\
  \end{tabular}
  \caption{Induced bilateral asymmetry due to tumor presence causing
  distortion of the plane of symmetry.  Shown are mid-axial slices of
  one of the BRATS 2013 training data 
  (specifically {\tt BRATS\_HG0001}):  (a) FLAIR, (b) T1, (c) T1C, and
  (d) T2.  }
  \label{fig:asymmetry}
\end{figure}

\begin{figure*}
    \centerline{\includegraphics[width=120mm]{Fig2.eps}}
  \caption{Multivariate symmetric template created from the MMRR 
           data \citep{landman2011}.  Of the seven modalities 
           comprising the set of study acquisitions, we illustrate the
           (a) FA, (b) FLAIR, (c) MPRAGE, and (d) T2 template components.
           Although DWI-based images were not included in the challenge data, 
           such images have shown discriminative potential \citep{price2003,cha2005} 
           warranting investigation in our future work.  
           The optimal transformation and averaging of the individual 
           subject images result in the asymmetric template represented at 
           the top of the middle column.  A horizontal reflection, $T_R$,
           perpendicular to the mid-sagittal plane  resulted in the 
           contralateral counterpart represented
           at the bottom of the second column.  The final template seen on
           the right is a result of repeating the template construction using
           the two asymmetric templates as input.
          }
  \label{fig:symmetrictemplates}
\end{figure*}


To take into account these potential asymmetries and the distortion of 
the normal plane of symmetry in the brain, we 
require a data set with the same modalities as dictated
by the subject image acquisition protocol.  Although it
is preferable to build population-specific multivariate
templates from normal data using the same acquisition 
parameters \citep{avants2010}, such data were not available
from the BRATS challenge.  Therefore, we substituted well-known,
publicly available data from a recent neuroimaging reproducibility study
by Landman et al. which resulted in an open data cohort of 21
normal individuals, each imaged twice, comprising several
modalities including ASL, FLAIR, DTI, fMRI, T1, and T2 
\citep{landman2011}.  These data (known as the
``MMRR'' data set) were selected for deriving
a multivariate template due to its public availability and
inclusion of several modalities even permitting future 
incorporation of modalities 
not currently included with the BRATS challenge into our 
segmentation framework.  


\begin{figure}
  \centering
    \includegraphics[width=100mm]{Fig3.eps}
  \caption{ Given the mapping between the template, $T$, and subject, $S$, domains ($\phi_b: S  \leftrightarrow \underset{b}{\leftrightsquigarrow} T$), various features can be calculated which demonstrate good discriminative qualities.  $T_R$ denotes a horizontal reflection
  perpendicular to the mid-sagittal plane.
  Feature images used are specified by the dashed box.
Top:  Difference images with the symmetric multivariate template are created by warping the template to the subject space and performing a voxelwise subtraction from the original modality image.
Bottom:  Similarly, contralateral difference images are calculated from each modality per subject by generating the non-Euclidean contralateral image via the diffeomorphic transform $\phi_b$.  
          }
  \label{fig:asymmetryFeatures}
\end{figure}

As detailed in \cite{avants2008,avants2010}, 
given $K$ multimodality images, ${\mathbf I} = \{I_1,I_2,\ldots, I_K\}$, 
for $N$ subjects,  multivariate 
template construction iterates between optimizing the set 
of diffeomorphic transforms between the subjects and the 
template, 
$\left\{\left(\phi_1,\phi_1^{-1}\right),\ldots,\left(\phi_N,\phi_N^{-1}\right)\right\}$ 
and constructing the 
optimal multivariate template appearance 
$\mathbf{J}=\{J_1,J_2,\ldots, J_K\}$, with corresponding 
coordinate system $\psi(\mathbf{x})$, to minimize the
following cost function:
\begin{align}
  \label{test}
  \sum_{n=1}^N \Bigg[ &D \left( \psi(\mathbf{x}),\phi_1^n(\mathbf{x},1)\right) \nonumber  \\
           &+ \sum_{k=1}^K \lambda_k \Pi_k \left(I_k^n,J_k   
           \left(\phi^{-1}_n(\mathbf{x},1)\right)\right)\Bigg].
\end{align}
%\begin{align}
%  \sum_{n=1}^N 
%        \left[ D \left( \psi(\mathbf{x}),\phi_1^n(\mathbf{x},1)\right)   
%        + \sum_{k=1}^K \lambda_k \Pi_k \left(I_k^n\left(\phi_n(\mathbf{x},0.5)\right),J_k\left(\phi^{-1}_n(\mathbf{x},0.5)\right)\right)\right]
%\end{align}
$D$ is the diffeomorphic shape distance,
\begin{align}
D\left( \phi( \mathbf{x},0),\phi( \mathbf{x},1)\right) = \int_0^1 \| \nu(\mathbf{x},t)\|_L dt
\end{align}
dependent on the choice of linear operator, $L$, and $\nu$
is the velocity field
\begin{align}
\nu\left( \phi(\mathbf{x},t) \right) = \frac{d\phi(\mathbf{x},t)}{dt},\,\,\, \phi(\mathbf{x},0) = \mathbf{x}.
\end{align}
Each pairwise registration employing the similarity metric $\Pi_k$ can 
be assigned a relative weighting, $\lambda_k$, to weight a particular
modality's influence in the construction process.  Once the multivariate
template has converged (typically in four iterations), we symmetrize
the template by flipping each asymmetric template component contralaterally 
and then running the
multivariate template construction a second time using only the multivariate
template and its symmetric analog.  This is illustrated conceptually in
Figure \ref{fig:symmetrictemplates}.

%In terms of implementation, this template building algorithm is 
%encapsulated in the script \verb#antsMultivariateTemplateConstruction.sh#,
%available in the ANTs repository, which permits serial or parallel processing on
%an individual workstation or on a computational cluster.  In
%Figure \ref{fig:symmetrictemplates} we show mid-axial slices from
%the MMRR multivariate symmetrical template consisting of FA%
%\footnote{
%Although DWI-based images were not included in the
%challenge data, such images have shown discriminative potential 
%\citep{price2003,cha2005} warranting investigation in our future
%work.  
%}, FLAIR,  
%T1, and T2 components.  These images have been made publicly available
%along with other processed images such as corresponding
%tissue priors and structural labels.%
%\footnote{
%http://figshare.com/articles/Kirby\_multivariate\_template/852989.
%All 7 components are stored under the more informal ``Kirby'' data set moniker.
%}

\subsection{Image Features Based on Asymmetry and Symmetric Template Normalization}

After constructing the template offline, each data set is processed
by first registering the non-contrast T1-weighted image to the T1-weighted
component of the symmetric template.  To do this we use a recently developed 
SyN \citep{avants2011a} variant based on B-spline regularization 
which has demonstrated good performance in normal brain registration \citep{tustison2013a}.
%Good performance also extends to this pathological data scenario as 
%indicated by visual inspection and the fact that the derived features
%were amongst the most informative in our winning entry.
We denote the mapping from the subject, $S$, to the template, $T$,
space as $\phi_b: S \leftrightarrow \underset{b}{\leftrightsquigarrow}  T$
which consists of both affine and diffeomorphic components.  Note that transform
invertibility is essential for the template-based features.  The first 
set of feature images is generated by warping the template components 
to the subject space and calculating the difference image (see the 
top portion of Figure \ref{fig:asymmetryFeatures}).  
Note that the T1-weighted contrast and non-contrast images are paired with
the T1-weighted component of the symmetric template.
For example,
the T2 symmetric template voxelwise difference image is calculated from
\begin{align}
  \mathrm{T2}\,\,\mathrm{symmetric}\,\,\mathrm{template}\,\,\mathrm{difference} = S_{\mathrm{T2}} - T_{\mathrm{T2}}\left(\phi_b^{-1}\right).
\end{align}

The second
set of template-based feature images is generated per modality 
as the difference image with the contralateral reflection.  This is achieved by calculating the 
reflection transform, $T_R$, in the symmetric template space and composing transforms
as follows to create the non-Euclidean contralateral counterpart per modality:
\begin{align}
  S_{contralateral} = S\left( \phi_b^{-1}\left( T_R\left( \phi_b \right)\right)\right).
\end{align}
The corresponding feature image is calculated as the voxelwise difference $S_{contralateral} - S$.
This process is illustrated in the bottom portion of Figure \ref{fig:asymmetryFeatures}. Two 
related features are the Jacobian determinant image 
of the transform (calculated voxelwise) from the template to the subject, i.e., $J(\phi_b^{-1})$
and the warped signed distance template mask \citep{maurer2003}.
The motivation for inclusion of these two feature is that 
relatively larger Jacobian values are a potential indicator of tumor
expansion similar to the distorted distance map.


\begin{figure*}
\centering
  \makebox[0.9\textwidth][c]{
  \includegraphics[width=120mm]{Fig4.eps}
  }
  \caption{Representative feature images derived from the 
           BRATS\_HG0301  final assessment data set.
           Neighborhood statistical images for each modality were 
           generated by calculating a given statistic within
           a specified neighborhood radius.  Similarly, we 
           calculate the voxelwise difference image between the
           T1 and T1C image.  
           of 
           Also calculated for each modality were feature
           images based on either the GMM or the MAP-MRF segmentation.  For the former, we
           show the probability maps for each of the seven labels which are used as feature
           images.  From the resulting hard segmentation, we calculate various geometric 
           measures per connected component of each of the seven labels.  Similarly, the 
           registration to the symmetric template produces the modality-specific 
           difference images with the
           corresponding symmetric template itself and with respect to the 
           contralateral side.   This mapping is also used to produce the log Jacobian image
           and warped template distance mask.  Additionally, the 
           (T1 - T1C) image is calculated and, from the
           cerebral mask, we calculate the normalized distance image.  
           }
  \label{fig:featureImages}         
\end{figure*}

\subsection{Voxelwise Image Features for Random Forest Supervised Segmentation}

\begin{figure}
  \centering
   \vspace{-15mm}
  \makebox[0.9\textwidth][c]{
    \includegraphics[width=150mm]{Fig5.eps}
    }
  \caption{Diagrammatic workflow for the proposed RF model training (left) and prediction
  (right).  For training, the feature images are first generated from the set of input 
  training multimodal MRI and symmetric multivariate template. The set of features images are 
  use to create the RF model for the first stage.  This first stage RF model is then applied
  to the set of input training data to yield spatial priors for seeding the generation of the
  second stage feature images.  These feature images are then used to create the second
  stage RF model.  For prediction of a single-subject segmentation, a similar scenario 
  is applied whereby the generated feature images are used as input to the RF models the
  second of which produces an initial segmentation estimate.  This estimate is then refined
  using a series of heuristically-derived binary morphological operations resulting in the 
  final labeled brain.
  }
  \label{fig:pipeline}
\end{figure}

We use random forests as supervised learners.  Given ground truth,
voxel-wise $n$-class labels for a specific image, we can vectorize the
label image to produce $v_L$, the $p$-length vector of class labels
where $p$ is the number of voxels in the given brain of interest.
We also assume a matrix of features for the given brain, which we
denote $\bm{F}$ where $\bm{F}$ has dimensionality of $p$ by
$k$, where each of the $k$ columns corresponds to a potentially valuable
feature vector.  Given this representation, the random forest model
can be trained, in \textit{R} notation, to predict the labels via
$v_L \sim  \bm{F}$.
Our segmentation protocol involves training and application
of two RF models in succession 
(see Figure \ref{fig:pipeline}) to which we refer as ``stages.''  The basic idea is that
we generate a set of feature images used as input to the
first RF model (or first stage) which produces a voxelwise 
probabilistic tissue estimate.  More precisely, in the RF framework, each prediction sample (i.e. the feature vector at each voxel),
is propagated through each tree of the ensemble where it is labeled as belonging to a specific
class.  
These ``votes'' are converted to voxelwise probabilistic
estimates for each class via standard mechanisms used in random forest
models, e.g. \cite{liaw2002}.
We then use these output tissue map estimates as spatial 
priors for generating a second set of geometrically refined image features.  These
are used as input for a second RF model application (or
second stage),
the output of which constitute the final tumor segmentation estimate.
Note that some feature images are included directly and
  without modification in both stages such as
the asymmetry features described in the previous section.


\subsection{Image Features Based on Intensity Modeling and Regional Geometry}

Intensity modeling of the constituent tissue types has proven
useful in previous tumor segmentation protocols.  For example,
Gaussian mixture modeling (GMM) was used in \citep{bauer2012,zikic2012} to model
tumor tissue components.  Similarly, we use GMM to model 
the seven brain/tumor tissue types:
\begin{itemize}
\item cerebrospinal fluid,
\item grey matter,
\item white matter,
\item edema, 
\item non-enhancing tumor (including low-grade tumor center), 
\item enhancing tumor (excluding necrotic center), and 
\item abnormal necrotic center or necrocyst in high-grade gliomas.
\end{itemize}
However, since GMM is performed without any spatial 
considerations, we augment the initial supervised segmentation 
classification result by employing a second classification round
using an MAP-MRF prior on the same seven brain/tumor tissue types
(detailed below).

In contrast to previous generative
modeling approaches for multimodal tumor segmentation 
\citep[e.g.,][]{prastawa2003}, we do not use multivariate 
Gaussians to specify tissue probabilities but rather incorporate each
univariate probability map into the feature vector of the training
data.  As pointed out in \cite{menze2010}, parametric multivariate modeling
might obscure the distinct biological information provided by each 
modality.  Instead, we let the RF construction 
process determine the optimal combination of such multivariate
information.  Additionally, maximum posterior labeling from both stages
is used to determine the connected components for each label.  
Geometric features (assigned voxelwise) include the physical volumes 
of each connected component, the volume to surface area ratio, 
the elongation, and eccentricity.  Note that random forests are
known to be robust to the presence of multiple correlated features.

\paragraph{Stage 1:  Voxelwise classification with Gaussian mixture
  modeling} 
The first step in processing a new image is to segment the image into
the $n$ tissue classes listed above.   We achieve this via an expectation-maximization
segmentation method based on
GMM.  GMM results in voxelwise
tissue probabilities which comprise an additional set of image
features.  The GMM is initialized with prior cluster centers for
specific tissue types.  We learn the values for these cluster centers
for each modality and each tissue from training data
\citep{reynolds2009}.  We also perform intensity normalization
across the training cohort by winsorizing the intensity values to the quantile
range $[0.01, 0.99]$ and then rescaling the resulting intensities to 
$[0, 1]$.  The cluster centers are defined as the mean
normalized intensity value for each tissue type of each modality 
image over all the training data.  We tried a more sophisticated intensity
normalization scheme using the approach of \cite{nyul2000} but found that
our simpler normalization approach, in combination with the rest of the
pipeline, produced slightly better results.  

More formally, the GMM computes the 
probability distribution at each voxel, $\mathbf{x}$, as the
sum of $M$ Gaussian components, $\mathcal{N}(\mathbf{x}|\mu,\sigma)$, i.e.
\begin{align}
p\left(\mathbf{x}|\mu_m,\sigma_m,\lambda_m\right) = \sum_{i=1}^M \lambda_m \mathcal{N}(\mathbf{x}|\mu_m,\sigma_m)
\end{align}
where $\sum_{m=1}^M \lambda_m = 1$.  The parameters of the GMM 
are determined using the Atropos segmentation tool \citep{avants2011} available in ANTs.
For the BRATS data, GMM modeling was applied to each of the four modalities, i.e.,
FLAIR, T1, T1 contrast, and T2 (see Figure \ref{fig:stageComparison}), separately.  
Note that no spatial priors are used during this stage including MRF spatial priors.

%\paragraph{Stage 1 random forest} 
The stage 1 random forest training uses the asymmetry features, the seven posterior probability
tissue images output by the GMM
along with five additional geometric features based on the connected 
components of the segmentation image.  These are created by first isolating
each of the seven tissue labels.  For each label we determine the connected
components then, for each connected component, we calculate its physical
volume and volume-to-surface ratio. Additionally, we calculate the component's 
second-order central image moments assembled into the image covariance matrix 
\citep{padfield2008}.  
The ordered eigenvalues are denoted as $\{\lambda_1,\lambda_2,\lambda_3\}$ and 
are used to calculate a 3-D estimate of the eccentricity,
\begin{align}
  eccentricity = \frac{\lambda_3 - \lambda_1}{\lambda_3},
\end{align}
and elongation,
\begin{align}
  elongation = \frac{\lambda_3}{\lambda_1},
\end{align}
meant to discriminate isotropic vs. anisotropic objects.
Note that each measure applies to a single connected component but
the value is assigned to each voxel comprising that labeled object.
For each image, we also calculate the Euclidean distance
to the tumor core (label 7).  

During the training phase (or RF model construction),
these feature images, $\left\{F_i: i \in GMM\right\}$, generated during Stage 1 are used to create
the Stage 1 RF model with the following relationship
\begin{align}
\label{eq:gmm}
 v_L \sim \sum_{i \in Asym} F_i + \sum_{i \in GMM} F_i + \sum_{i \in Neigh} F_i + \sum_{i \in Dist} F_i
\end{align}
given in the standard \textit{R} notation of \cite{wilkinson1973}.  
In other words, $\bm{F}= [ \bm{F}_{Asym}, \bm{F}_{GMM},
\bm{F}_{Neigh}, \bm{F}_{Dist} ]$, where we column-wise concatenate the asymmetry ($Asym$),
intensity modeling and connected component geometry ($GMM$), neighborhood statistics ($Neigh$),
 and brain mask signed distances ($Dist$) feature matrices.
 
Application of the Stage 1 RF model to a set of feature images 
from an individual subject results in a set of $n$ spatial images
containing the voxel-wise probabilities for each 
of the seven tissue types.  At each voxel, the corresponding multi-feature 
vector is propagated through each tree of the random forest 
resulting in a single vote/classification per tree.  The
classification probabilities are derived by normalizing 
these votes to $[0,1]$ \citep{liaw2002}. 
 
%Therefore, in total, the number of features is $k=84$ where there
%are twelve GMM features (7 posteriors plus 5 geometric features) for each 
%of the four modalities ($n=48$), 
%two asymmetry features for each modality ($n=8$), and six modality-specific 
%miscellaneous geometric features ($n=24$) with an additional four feature
%images.

% 48 + 8 + 24 + 4 = 84

\paragraph{Stage 2:  Refined spatial coherence classification}
Intialization of stage 2 employs the per-class probabilities output
by the stage 1 RF model as input to an expectation-maximization MAP-MRF 
segmentation algorithm.  
 Specifically, these spatial probability maps are passed as priors to the
{\tt antsAtroposN4.sh} script in ANTs which couples the Atropos 
segmentation tool \citep{avants2011} with N4 bias correction \citep{tustison2010}.
The posterior probabilities that are output by Atropos and the
connected-component feature images for each modality  are denoted as $\bm{F}_{MRF}$.    
The second stage RF model is then produced similarly as the previous stage
using the concatenated feature set $\bm{F}= [ \bm{F}_{Asym}, \bm{F}_{MRF},
\bm{F}_{Neigh}, \bm{F}_{Dist} ]$: 
\begin{align}
\label{eq:mapmrf}
 v_L \sim \sum_{i \in Asym} F_i + \sum_{i \in MRF} F_i + \sum_{i \in Neigh} F_i + \sum_{i \in Dist} F_i.
\end{align}
The use of these MRF features improves the accuracy of the 
intensity-based modeling over the GMM approach by introducing
a spatial coherence constraint (the MRF prior) to those features 
in addition to 
providing a better initialization with the RF posteriors from 
the initial estimation stage (which lacked the MRF spatial prior).  
In total, the number of features is for this final stage is the same
where the GMM features are replaced by their MRF counterparts.
The rest of the feature images from the first stage are also included unaltered for this second stage.

%\subsubsection{Miscellaneous Multimodal Feature Image Generation}
%\label{sec:misc}
%Our strategy for feature selection was to, first, generate an ample feature
%set and, second, to use the training phase to prioritize the features
%appropriately.  In principle, this will eliminate weak features and
%retain strong ones even in the presence of substantial noise.  Given
%that image feature shape may be a valuable predictor of the presence of a
%tumor \cite{zacharaki2009classification}, we calculate, for each modality, several images of neighborhood 
%first-order statistics including mean, variance, skewness, and
%entropy.  Block neighborhood radii of values $\{1,2,3\}$ were tested.
%Due to overlapping discriminative qualities, we narrowed the feature
%values to 1 and 3 voxels.  We also calculated two Euclidean distance
%transforms \citep{maurer2003} to be used for each subject.  One was
%calculated from the subject's own cerebral mask and the other was the 
%distance transform of the symmetric template cerebral mask warped
%to the subject.  Finally, we also generated the  (T1 - T1C) difference 
%image \citep{prastawa2003}.
%The script {\tt createFeatureImages.sh} performs all preprocessing
%and generates all feature images.  In Figure \ref{fig:featureImages} we provide sample mid-axial 
%image slices from features generated from the {\tt BRATS\_HG0301} data
%set.  Note that these feature images do not require the presence of
%any training data.


\begin{figure}
  \centering
  \begin{tabular}{c}
    \includegraphics[width=88mm]{Fig6.eps}
  \end{tabular}
  \caption{Visual comparison between the first and second RF stages
  on the evaluation {\tt BRATS\_HG0001} data set.
  Although the GMM intensity modeling does provide certain 
  discriminative benefit (see Figures \ref{fig:hgimportance}
  and \ref{fig:lgimportance}), the use of the RF-derived posteriors 
  coupled with the described MAP-MRF segmentation framework enhances 
  the accuracy of the intensity-based modeling features.    
  }
  \label{fig:stageComparison}
\end{figure}


%\textcolor{red}{Nick, I think the motivation for this is still unclear
%  .... doesnt it have something to do with the MRF of atropos
%  smoothing out the importance features resulting from the RF step?
%  Figure 4 tells us a little bit about the procedure of the code but
%  the idea is not conveyed ... would it be possible to show exactly
%  what is happening in a separate figure?  e.g. just conceptually in a small 
%  region of voxels with 2 modalities ....?}


%\subsection{Overview}
%
%In summary, the proposed RF classification approach per subject consists of the 
%following steps:
%
%\begin{enumerate}
%  \item Calculate Stage 1 features based on:  
%  \begin{itemize}
%    \item GMM intensity modeling,
%    \item asymmetry, and
%    \item miscellaneous.
%  \end{itemize}
%  \item Apply Stage 1 RF model to the above calculated features resulting in seven 
%        probability maps for the seven tissue types.  
%  \item Use the probability maps to calculate Stage 2 features based on MAP-MRF intensity modeling.
%  \item Apply Stage 2 RF model to MAP-MRF, asymmetry, and miscellaneous features to 
%  get final RF classification. 
%\end{enumerate}

%\textcolor{red}{still feel unclear about it ...  does it go like this?
%\begin{enumerate}
%\item use tissue prior intensity models (not spatial) to initialize
%  Atropos multivariate EM-MRF segmentation of new image  w/o spatial
%  priors 
%\item get out segmentation and posteriors ... run some shape stats on
%  the segmentation 
%\item separately compute the asymmetry features by mapping to the MV
%  template 
%\item input  asymmFeatures + atroposPosteriors + shapeFeatures to the
%  RF model  ... with or w/o ground truth outcomes ?  same either way?
%  what do you get out of the RF?
%\end{enumerate}
%}









\subsection{\textit{ANTsR}:  An ANTs/R Interface for Random Forest Training and Prediction}

The complexity of neuroimaging research necessitates 
commensurable numerical analysis capabilities.  Similarly, concomitant
with the era of ``big data'' (specifically with respect to neuroimaging
\citep{vanhorn2013}) are new visualization needs and challenges
\citep{childs2013,kehrer2013}.
In response, various software packages have been developed to
integrate tools specific to neuroimaging research with more general
numerical and visualization software packages.
The well-known neuroimaging package SPM
%\footnote{
%http://www.fil.ion.ucl.ac.uk/spm/
%}
is a significant extension of the commercial computing and visualization environment
Matlab.  Open source neuroimaging packages, such as NIPY (neuroimaging in Python),
%\footnote{
%http://nipy.org
%} 
rely on other open source packages for numerical/statistical analysis.  NIPY,
for example, uses the more generic packages NumPy and SciPy for numerical analysis and 
optimization.
%\footnote{
%http://www.numpy.org
%}  

ANTs (Advanced Normalization Tools) was built, originally, to provide 
high performance image registration for medical image analysis
\citep{avants2008a} and based upon the mature Insight Toolkit (ITK)
sponsored by the National Institutes of Health.  Since then, ANTs has grown to include 
several robust medical image analysis solutions including bias 
correction \citep{tustison2010}, $n$-tissue multivariate segmentation 
\citep{avants2011}, template construction \citep{avants2010}, and cortical 
thickness estimation \citep{das2009} (many of which have been
introduced into ITK partially in an attempted leveraging of Linus's Law---``Given enough eyeballs, all bugs are shallow'').  
However, in the evolution of the toolkit, it became clear 
that robust statistical machinery was lacking for making inferences regarding
the data produced during the course of ANTs processing.  \textit{ANTsR} was developed
specifically to provide an interface between ANTs, a 
powerful neuroimaging toolkit for producing reliable imaging data 
transformations, and the \textit{R} project
%\footnote{
%http://www.r-project.org
%}
for statistical computing and visualization thus providing a complete
set of tools for multivariate neuroimage analysis.  
 \textit{ANTsR} intends to provide a modern framework for medical analytics, 
 with a focus on imaging-assisted prediction and statistical power.


Careful consideration of available statistical software 
led to the adoption of \textit{R} to complement ANTs quantification resulting in the
\textit{ANTsR} package.
%The \textit{R} project for statistical computing,%
%\footnote{
%www.r-project.org
%}
% or more compactly
%`R', is an environment for statistical computation
%and data visualization.  
\textit{R}'s open source code base, reliable software testing and distribution strategies,
and add-on packages coupled with its rapidly growing 
community of developers and users has caused wide-scale
adoption within both academia and industry.

\subsubsection{Installation}

The \textit{ANTsR} package is publicly available on the github project hosting service.%
\footnote{
\href{https://github.com/stnava/ANTsR}{https://github.com/stnava/ANTsR}
}
Prior to installation of \textit{ANTsR}, several external \textit{R} packages
need to be installed including: \verb#Rcpp#, \verb#signal#, \verb#timeSeries#, 
\verb#mFilter#, \verb#doParallel#, \verb#robust#, \verb#magic#, \verb#knitr#, \verb#pixmap#, 
\verb#rgl#, and \verb#misc3d#.
%\footnote{
%See \href{http://stnava.github.io/software/2014/01/08/antsr/}{http://stnava.github.io/software/2014/01/08/antsr/} for current status.}
Additionally, in order
to perform the supervised brain segmentation as described 
in later sections, one needs to also install the packages
\verb#randomForest#, \verb#snowfall#, \verb#rlecuyer#,
and \verb#ggplot2#.
%\footnote{
%Packages are easily installed using the {\tt install.packages()} \textit{R} mechanism.
%} 

In addition to \textit{R} and the add-on packages previously mentioned, CMake is also 
required.  CMake
%\footnote{
%http://www.cmake.org/
%}
is an open source tool for the management and building of 
large-scale software projects.  It is used
to coordinate the downloading of external packages,
such as the Insight Toolkit (ITK)
%\footnote{
%http://www.itk.org/
%}
and ANTs.  Further instructions for download and
installation can be found on the \textit{ANTsR} github website.  Feel
free to contact the authors if installation trouble occurs.  We note
that \textit{ANTsR} is currently only tested on UNIX-alikes such OSX and Ubuntu
operating systems.
%\footnote{Windows installation should be possible
%but, to our knowledge, has not been attempted.}

\subsubsection{Usage}
\textit{ANTsR} is intended to not only allow easy interchange between
medical imaging formats and \textit{R} but also to facilitate
reproducible scientific studies and the type compilable analysis
articles that are fundamental to journals such as
\textit{Biostatistics}.  Both \verb#knitr# and \verb#sweave#
facilitate integration of R-code with the LaTeX document
preparation system.  

An additional motivation for our development of \textit{ANTsR} (and
hopefully its acceptance by the community) 
stems from the ability to couple ANTs core 
functionality, including IO tools such as \verb#antsImageRead#, 
with the large number of \textit{R} statistical and
visualization packages.  Due to this combination, several
functions have been easily created for such neuroimaging-specific 
tasks as fMRI/ASL data manipulation and analysis,
voxel and ROI-based  analyses,
%(e.g. \verb#filterMRIforNetworkAnalysis#, \verb#aslPerfusion#)
and connectivity visualization. % (e.g. \verb#plotBasicNetwork#).
The user help menu and documentation for the library  and its
constituent functions are invoked in the similar manner as other
R libraries.

As mentioned earlier, we have made this entire framework
available as open source.  In addition to the \textit{ANTsR} repository
already on github which houses both ANTs and \textit{ANTsR} functionality, 
we created a special github repository specifically for this work
containing figures, references, and text.%
\footnote{
\href{https://github.com/ntustison/ANTsAndArboles}{https://github.com/ntustison/ANTsAndArboles}
}
Also, we posted all
scripts (R, shell, and perl) used to coordinate the \textit{ANTsR} processing 
including:
\begin{itemize}
  \item {\tt applyModel.R}:  applies a RF model to a new 
  feature data set from a testing subject resulting in a set of probability
  images (one for each label).
  \item {\tt applyTumorSegmentationModel.sh}:  generates the new feature image set 
  from the testing MRI (by calling {\tt createFeatureImages.sh}).
  organizes the file names in a csv file (via {\tt createCSVFileFromModel.R}),
  and applies the RF model using {\tt applyModel.R}. 
  \item {\tt applyTumorSegmentationModelForCohort.pl}:  Coordinates tumor 
  segmentation on the computational cluster for a given cohort.
  \item {\tt createCSVFileFromModel.R}:  organizes the set of feature image
  file names in a csv file for input into {\tt applyModel.R}.
  \item {\tt createFeatureImages.sh};  creates the set of feature images given
  a set of co-registered input MRI from a single subject. 
  \item {\tt createModel.R}:  creates a RF model given the input csv 
  file of the feature image file names for all training data and set of truth
  label maps.
  \item {\tt plotVariableImportance.R}:  produces a plot of the importance 
   of each feature variable used 
  in constructing the model.
\end{itemize}
We also include a fully functional 2-D example which performs
both testing and training on sample challenge data.
%\footnote{
%https://github.com/ntustison/ANTsAndArboles/tree/master/SimpleExample
%}
After pulling the repository, one can run the scripts {\tt exampleTrain.sh}
and {\tt examplePredict.sh} to get the sample results.  Output includes
several overlap measures describing the performance.

\subsection{Brain Tumor Data}

Brain tumor image data used in this work were obtained from the NCI-MICCAI 2013 
Challenge on Multimodal Brain Tumor Segmentation
%\footnote{
%http://martinos.org/qtim/miccai2013/index.html
%}
organized by K. Farahani, M. Reyes, B. Menze, E. Gerstner, J. Kirby and J. Kalpathy-Cramer. 
The challenge database contains fully anonymized images from the following institutions: 
ETH Zurich, University of Bern, University of Debrecen, and University of Utah and 
publicly available images from the Cancer Imaging Archive (TCIA).  Both training 
and testing data were made freely available through the Creative Commons Attribution-NonCommercial 3.0 license.

Training data consisted of multimodal brain MRI (T1, T2, FLAIR, and 
post-Gadolinium T1) from 30 glioma patients (both low, $n=10$, and high-grade, $n=20$,
and with and without resection).  For each subject, the T1, T2, and 
FLAIR MRI were linearly registered to the post-contrast T1.  Subsequently,
the brains were skull-stripped and resampled to 1 mm isotropic resolution.
Testing data was processed similarly and released during the course of the
challenge in two sets denoted as ``Leaderboard'' and ``Challenge'' data.  
The former consisted of 21 and 4 high and low-grade tumor patients, respectively,
whereas the latter comprised 10 high-grade only patients.

Manual labeling was performed in the axial plane following a detailed
protocol.
%\footnote{
%http://martinos.org/qtim/miccai2013/data.html
%}
The labeling of pathology was categorized into four regions:
edema, non-enhancing tumor (including low-grade tumor center), 
enhancing tumor (excluding necrotic center), and abnormal
necrotic center or necrocyst in high-grade gliomas.
Normal brain tissue was not labeled. 

\subsubsection{Training: RF Creation for the BRATS 2013 Challenge}

For use with the Challenge and Leaderboard data, cohort-specific models (both 
low-grade and 
high-grade glioma) for both GMM and MAP-MRF stages were created using only the 
supplied training data.  Prior to training, we segmented normal brain tissue \citep{avants2011}
for each data set by segmenting only the T1 image.  This was only to yield
a rough estimate of normal brain tissue to augment the already provided 
pathology labels.  This resulted in seven labels for tissues described
earlier i.e., csf, gray matter, white matter,
necrosis, edema, enhancing, and non-enhancing tumor characterizing each brain.

Initial testing of our proposed framework was performed 
on the training data using a leave-one-out strategy.  Once the
feature images are created for each subject, the resulting images of the entire
training cohort are organized in a csv file for input into the \textit{R} script
{\tt createModel.R}.  Other possible input parameters include the requested 
number of trees, number of samples per label, and number of threads for parallel
processing.  The output is an {\tt RData} file describing the RF
model which can be used for future predictions.
 
Additionally, the {\tt randomForest} package provides  measurements 
for determining the importance of chosen features when constructing the model.  
This aids in potential feature pruning or intuiting model behavior.  As mentioned
earlier, we provide the \textit{R} script {\tt plotVariableImportance.R} to render
one such quantity use to assess RF importance using {\tt MeanDecreaseAccuracy}.  
During model construction
(specifically the out-of-bag error calculation stage), the decrease in prediction accuracy
with the omission of a single feature or variable is tracked and averaged.  Thus,
those features which have the greatest decrease in mean accuracy are considered
to be the most discriminative.
%\footnote{
%There is an issue with consistently ranking correlated predictors as described at {\tt http://www.r-bloggers.com/random-forest-variable-importance/} since the permutation testing performed for predictive accuracy assessment assumes predictor independence.  Correctives have been proposed but we ignore these issues for this particular application.
%}
In this work, we do not use these measurements for feature pruning.  However,
we plot them in the Results section (see Figures \ref{fig:hgimportance} and
\ref{fig:lgimportance}) as they demonstrate the relative importance of our
selected features including that of the proposed asymmetry images.

\subsubsection{Prediction:  Applying the RFs for the BRATS 2013 Challenge}

Once the models are created, classification of tumors in new subjects is performed
as illustrated in Figure \ref{fig:pipeline}.  From the feature images and input 
GMM model, a tentative set of RF voting output confidence images are produced.
As described, this is used as input to the second prediction round.  The 
final probability output images are used to produce the maximum probability labeling.  

A final round of binary morphological operations were heuristically designed
to improve the final segmentation results such as removal of small connected 
components and morphological closure of certain regions.
All steps are included in the script 
{\tt applyTumorSegmentationModelForCohort.pl} designed for parallel subject 
processing on the computational cluster at the University of Virginia.
%\footnote{
%http://www.uvacse.edu
%}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Results
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}


\begin{figure*}
  \vspace{-20mm}
  \makebox[\textwidth][c]{
  \includegraphics[width=140mm]{Fig7.eps}
  }
  \caption{Visual results (every tenth slice sampled from the axial center of the
  tumor region) from the BRATS 2013 challenge using the proposed
  framework (off-white = non-enhancing tumor, cyan = edema, violet = enhancing 
  tumor, yellow = necrosis).  Tumor components were combined into
  three regions for performance assessment ((1) complete tumor: all four tissue
  classes; (2) tumor core:  necrosis, non-enhancing tumor, and enhancing tumor;
  and (3) enhancing tumor).  See Table \ref{table:results} for quantitative measures
  and challenge rankings.
  }
  \label{fig:challengeresults}
\end{figure*}




%\begin{table*}
%\caption{Results from the MICCAI 2013 BRATS Challenge}
%\label{table:results}
%\begin{center}
%\begin{tabular*}{0.99\textwidth}{@{\extracolsep{\fill} } c c c c c c c c c c c}
%\toprule
%{\bf Data set} & {\bf Rank} & \multicolumn{3}{c}{\bf Dice} & \multicolumn{3}{c}{ \bf Positive Predictive Value} & \multicolumn{3}{c}{ \bf Sensitivity} \\
%{} & {} & complete & core & enhanced & complete & core & enhanced & complete & core & enhanced \\
%\midrule
%Challenge & 1 & 0.87 (1) & 0.78 (1) & 0.74 (1) & 0.85 (2) & 0.74 (5) & 0.69 (4) & 0.89 (2) & 0.88 (1) & 0.83 (1) \\
%Leaderboard & 1 & 0.79 (2) & 0.65 (1) & 0.53 (3) & 0.83 (1) & 0.70 (2) & 0.51 (3) & 0.81 (4) & 0.73 (2) & 0.66 (2) \\
%Evaluation & 2 & 0.88 (2) & 0.76 (2) & 0.55 (3) & 0.88 (3) & 0.80 (5) & 0.65 (3) & 0.89 (3) & 0.79 (3) & 0.53 (3) \\
%\bottomrule
%\end{tabular*}
%\end{center}
%\end{table*}


\begin{table*}
\caption{Assessment measures from the MICCAI 2013 BRATS Challenge including
 Dice overlap, positive prediction value (PPV), and sensitivity.}
\label{table:results}
\begin{center}
\begin{tabular*}{0.99\textwidth}{@{\extracolsep{\fill} } l c c c c}
\toprule
\multicolumn{1}{c}{ \bf Data Set} & {\bf Rank}$^\dagger$ & {\bf Dice} & {\bf PPV} & {\bf Sensitivity} \\
\midrule
Challenge (complete) & 1 & 0.87 (1) & 0.85 (2) & 0.89 (2) \\
Challenge (core) & {} & 0.78 (1) & 0.74 (5) & 0.88 (1) \\
Challenge (enhanced) & {} & 0.74 (1) & 0.69 (4) & 0.83 (1) \\
\midrule
Leaderboard (complete) &  1 & 0.79 (2) & 0.83 (1) & 0.81 (4) \\
Leaderboard (core)     & {} & 0.65 (1) & 0.70 (2) & 0.73 (2) \\
Leaderboard (enhanced) & {} & 0.53 (3) & 0.51 (3) & 0.66 (2) \\
\midrule
Evaluation (complete) &  2 & 0.88 (2) & 0.88 (3) & 0.89 (3) \\
Evaluation (core)     & {} & 0.76 (2) & 0.80 (5) & 0.79 (3) \\
Evaluation (enhanced) & {} & 0.55 (3) & 0.65 (3) & 0.53 (3) \\
\bottomrule
\multicolumn{5}{l}{\scriptsize$^\dagger$Indicates total performance
 over all regions.  Component rankings are 
in parentheses.
}
\end{tabular*}
\end{center}
\end{table*}


A total of four RF models were created from the 30 training data 
sets. Two models for Stage 1 and Stage 2 processing were generated from the 
20 high-grade glioma evaluation data described earlier.  Similarly, two
additional models were created from the 10 low-grade glioma data sets.  Following
model construction, weighted importance feature plots described earlier were 
rendered to provide feedback as to the individual potential predictive accuracy
of each feature.  The high-grade plots are given in Figure \ref{fig:hgimportance}
whereas the low-grade plots are given in Figure \ref{fig:lgimportance}.

\begin{figure*}
  \makebox[\textwidth][c]{
  \begin{tabular}{cc}
  \includegraphics[width=80mm]{Fig8a.eps} &
  \includegraphics[width=80mm]{Fig8b.eps} \\
  \end{tabular}
  }
  \caption{{\tt MeanDecreaseAccuracy} plots generated from the high-grade glioma
  Stage 1 and Stage 2 RF models.  These plots provide a weighted 
  ranking describing the importance of each feature for predictive accuracy. 
  }
  \label{fig:hgimportance}
\end{figure*}

\begin{figure*}
  \makebox[\textwidth][c]{
  \begin{tabular}{cc}
  \includegraphics[width=80mm]{Fig9a.eps} &
  \includegraphics[width=80mm]{Fig9b.eps} \\
  \end{tabular}
  }
  \caption{{\tt MeanDecreaseAccuracy} plots generated from the low-grade glioma
  Stage 1 and Stage 2 RF models.  These plots provide a weighted 
  ranking describing the importance of each feature for predictive accuracy.
  }
  \label{fig:lgimportance}
\end{figure*}

Immediately apparent from these plots are the importance of certain features.
In general, the features based on the symmetric template are quite
discriminative thus justifying the increased computational 
resources required to generate such features.  For single-threaded processing
on the cluster, creating the feature images for a single subject required 
approximately 2 hours of total processing time of which the template registration 
component took approximately 75\%.  Additionally, the first order 
statistical features also seemed fairly important which is something that
previous work had demonstrated \citep[e.g.,][]{bauer2012,zikic2012}.
There are a number of differences, however, between low-grade and high-grade 
importance outcomes which would also seem to justify the creation of separate
glioma class models.

As described earlier, the MICCAI 2013 BRATS challenge data was provided to the
competitors in three sets.  The Evaluation set was used primarily for training
although an initial ranking was performed for all the competitors to aid in
determining participation in the workshop.  Shortly prior to the actual workshop
date, the Leaderboard data was released to the competitors for posting of results
and subsequent ranking.  Finally, the night before the competition, the Challenge
data was made available and used to produce the final competitor ranking.  In addition
to the Dice overlap measure, additional performance emeasures for producing the rankings 
included the positive predictive value, and sensitivity (all of which can be calculated
using open source tools such as \cite{tustison2009}).  
For all three data sets, we provide these overlap measures and relative competitor 
ranking in Table \ref{table:results}.  Full competition results can be viewed
at {\tt http://www.virtualskeleton.ch}.

\section{Discussion and Conclusions} 

One of the difficulties with the focus of this particular competition 
is the lack of consensus, even clinically, as to tumor type and extent
in the context of medical imaging \citep{cha2005}.  In fact, current 
consensus guidelines from the World Health Organization for brain 
tumor classification are strictly histopathological \citep{louis2007}
which limit clinical application.  Such limitations motivate the
use of medical imaging for treatment planning and outcome analyses 
\citep{cha2005} including more automated methods such as that proposed
in this work.

Although previous research has employed RFs for supervised brain
segmentation, our contribution of concatenated RF model application and use
of symmetric multivariate templates demonstrated good performance 
in the recent MICCAI 2013 Brain Tumor Segmentation challenge/workshop.  
In terms of variable importance, the latter provided several highly 
discriminative features which resulted in the top-performing algorithm 
of the competition.  This confirmed what others have found in that
RFs provide an excellent framework for prediction in certain
medical image analysis problems.  Even with relatively few training
subjects relative to input variables, the RF models
perform well.
In addition, this work highlights the value of  a symmetric multiple
modality template for clinical disorders in which asymmetry is a
hallmark.  Furthermore, our challenge-leading results establish the
value of such templates in multiple modality prediction
tasks in which there is a small training set with an abundance of
multivariate data.

Objective assessment  of algorithmic performance (given the prevalence of selection bias
in reporting results in conventional publication venues) is certainly a motivating
factor for these competitions. Equally, if not more, important
is the availability of the actual algorithmic instantiation (i.e. code) so that
others can more easily build upon and utilize what has proven effective. 
This also provides an opportunity for the users to return
constructive feedback to the authors thereby improving the original offering.  

However, as explained previously, the brain tumor segmentation 
methodology that we have made available is only a small part of the 
larger software package that we have created in \textit{ANTsR}.  Not only does
\textit{ANTsR} significantly facilitate the development of the work discussed,
but it provides an interface to one of the most powerful statistical
packages available in R.  The combination of the well-known ANTs software
package with \textit{R} provides tremendous potential for future insightful analysis.









\section*{Disclosures}
The authors declare that they have no conflict of interest.

%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base

\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliography{references}

\end{document}
% end of file template.tex

